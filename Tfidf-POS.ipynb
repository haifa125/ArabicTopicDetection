{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT 469 - Phase 1 (Code)\n",
    "### Group #3 - Topic Detection\n",
    "<B>Student Names:</B> Amal - Haifa - Reem - Waad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Packages and Importing Libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install gensim spacy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "#spacy and genism must be installed before\n",
    "import spacy\n",
    "import gensim\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Definded Functions\n",
    "<b>1-</b> extract(path) --> a function used to extract files from a specified directory and store the files in a list. We use this function to concat all the files of the data \n",
    "<br><br>\n",
    "<b>2-</b> clean(text) --> a function used to clean the text passed to it.\n",
    "<br><br>\n",
    "<b>3-</b> WriteToFile(path,textfile) --> a function used to store the text to .txt files in a specified directory. We use this function to save the cleaned to text to .txt files, then we pass the .txt files to MadaMira in the terminal.\n",
    "<br><br>\n",
    "<b>4-</b> ReadFromFile(path) --> a function used to read the text files from the specified directory. We use this function to read the files after we passed it to MadaMira and store them into variables, to be used later in the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "#returns the names of the files in the directory data as a list\n",
    "def extract(path):  \n",
    "    list_of_files = os.listdir(path)\n",
    "    list1=[]\n",
    "    for file in list_of_files:\n",
    "        x = path+'/'+file\n",
    "        f = io.open(x, 'r+',encoding=\"utf-8\", errors='ignore')\n",
    "    #append each line in the file to a list\n",
    "        list1.append(f.read())\n",
    "        f.close()\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\" \n",
    "    \"]+\" , flags=re.UNICODE)\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "\n",
    "                         \"\"\", re.VERBOSE)\n",
    "def clean(text): \n",
    "    \n",
    "    #Remove URLs\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+', ' ', text)\n",
    "    \n",
    "    #Remove emails\n",
    "    text = re.sub('^\\w+@[a-zA-Z_]+?\\.[a-zA-Z]{2,3}$', '', text)\n",
    "    \n",
    "    #Remove mentions (if any)\n",
    "    text = re.sub('@[A-Za-z0–9]+', ' ', text)\n",
    "    \n",
    "    #Remove emojis\n",
    "    text = re.sub(emoji_pattern, \"\", text)\n",
    "    \n",
    "    #Remove emoji encodings\n",
    "    text = re.sub('emoji[0-9][0-9][0-9]', '', text) \n",
    "    \n",
    "    #Remove Arabic diacritics\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "\n",
    "    #Remove English letters \n",
    "    if re.match(\"#(\\w+)\",text) == None: \n",
    "        text = re.sub(\"[A-Za-z]\", \"\", text)\n",
    "        \n",
    "    #Translate numbers from English to Arabic \n",
    "    text = re.sub(\"0\", \"٠\", text)\n",
    "    text = re.sub(\"1\", \"١\", text)\n",
    "    text = re.sub(\"2\", \"٢\", text)\n",
    "    text = re.sub(\"3\", \"٣\", text)\n",
    "    text = re.sub(\"4\", \"٤\", text)\n",
    "    text = re.sub(\"5\", \"٥\", text)\n",
    "    text = re.sub(\"6\", \"٦\", text)\n",
    "    text = re.sub(\"7\", \"٧\", text)\n",
    "    text = re.sub(\"8\", \"٨\", text)\n",
    "    text = re.sub(\"9\", \"٩\", text)\n",
    "        \n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"[يى]\", \"ي\", text)\n",
    "    text = re.sub(\"[ئؤ]\", \"ء\", text)\n",
    "    \n",
    "    text = re.sub(\"[«»،؟,ـ]\", \"\", text)\n",
    "    \n",
    "    exclude1 = set(string.punctuation+'؟،٪؛') \n",
    "    exclude2 = set(string.punctuation+ '١٢٣٤٥٦٧٨٩٠؟،٪؛')\n",
    "    text = ''.join(ch for ch in text if ch not in exclude1) \n",
    "    text = ''.join(ch for ch in text if ch not in exclude2) \n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "#This code reads the list of text and stores the values of each index in the list inside one text file\n",
    "#We did this once for each topic list and for each verstion after cleaning the text to pass them to MadaMira\n",
    "def WriteToFile(path,textlist):\n",
    "    try:\n",
    "        with open(path, 'w') as f:\n",
    "            for text in textlist:\n",
    "                f.write(\"%s\\n\" % text)\n",
    "            f.close()\n",
    "\n",
    "    except (IOError, ValueError) as e:\n",
    "        print(\"error\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "# and now after we passed each clean text to madamira we will read them again and store them in variales\n",
    "# to be used later in the modelling section \n",
    "def ReadFromFile(path):\n",
    "    f = open(path, \"r\")\n",
    "    text = f.read()\n",
    "    textlist = text.split(\"\\n\")\n",
    "    f.close()\n",
    "    return textlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Data Acquisition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version1\n",
    "Art = extract('../../Datasets/dataset/version1/Art')\n",
    "Economy = extract('../../Datasets/dataset/version1/Economy')\n",
    "Health = extract('../../Datasets/dataset/version1/Health')\n",
    "Law = extract('../../Datasets/dataset/version1/Law')\n",
    "Literature = extract('../../Datasets/dataset/version1/Literature')\n",
    "Politics = extract('../../Datasets/dataset/version1/Politics')\n",
    "Religion = extract('../../Datasets/dataset/version1/Religion')\n",
    "Sport = extract('../../Datasets/dataset/version1/Sport')\n",
    "Technology = extract('../../Datasets/dataset/version1/Technology')\n",
    "\n",
    "#Version2\n",
    "Art2 = extract('../../Datasets/dataset/version2/Art')\n",
    "Economy2 = extract('../../Datasets/dataset/version2/Economy')\n",
    "Health2 = extract('../../Datasets/dataset/version2/Health')\n",
    "Law2 = extract('../../Datasets/dataset/version2/Law')\n",
    "Literature2 = extract('../../Datasets/dataset/version2/Literature')\n",
    "Politics2 = extract('../../Datasets/dataset/version2/Politics')\n",
    "Religion2 = extract('../../Datasets/dataset/version2/Religion')\n",
    "Sport2 = extract('../../Datasets/dataset/version2/Sport')\n",
    "Technology2 = extract('../../Datasets/dataset/version2/Technology')\n",
    "\n",
    "#Version3\n",
    "Art3 = extract('../../Datasets/dataset/version3/Art')\n",
    "Economy3 = extract('../../Datasets/dataset/version3/Economy')\n",
    "Health3 = extract('../../Datasets/dataset/version3/Health')\n",
    "Law3 = extract('../../Datasets/dataset/version3/Law')\n",
    "Literature3 = extract('../../Datasets/dataset/version3/Literature')\n",
    "Politics3 = extract('../../Datasets/dataset/version3/Politics')\n",
    "Religion3 = extract('../../Datasets/dataset/version3/Religion')\n",
    "Sport3 = extract('../../Datasets/dataset/version3/Sport')\n",
    "Technology3 = extract('../../Datasets/dataset/version3/Technology')\n",
    "\n",
    "#Version4\n",
    "Art4 = extract('../../Datasets/dataset/version4/Art')\n",
    "Economy4 = extract('../../Datasets/dataset/version4/Economy')\n",
    "Health4 = extract('../../Datasets/dataset/version4/Health')\n",
    "Law4 = extract('../../Datasets/dataset/version4/Law')\n",
    "Literature4 = extract('../../Datasets/dataset/version4/Literature')\n",
    "Politics4 = extract('../../Datasets/dataset/version4/Politics')\n",
    "Religion4 = extract('../../Datasets/dataset/version4/Religion')\n",
    "Sport4 = extract('../../Datasets/dataset/version4/Sport')\n",
    "Technology4 = extract('../../Datasets/dataset/version4/Technology')\n",
    "\n",
    "#Version5\n",
    "Art5 = extract('../../Datasets/dataset/version5/Art')\n",
    "Economy5 = extract('../../Datasets/dataset/version5/Economy')\n",
    "Health5 = extract('../../Datasets/dataset/version5/Health')\n",
    "Law5 = extract('../../Datasets/dataset/version5/Law')\n",
    "Literature5 = extract('../../Datasets/dataset/version5/Literature')\n",
    "Politics5 = extract('../../Datasets/dataset/version5/Politics')\n",
    "Religion5 = extract('../../Datasets/dataset/version5/Religion')\n",
    "Sport5 = extract('../../Datasets/dataset/version5/Sport')\n",
    "Technology5 = extract('../../Datasets/dataset/version5/Technology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(300):\n",
    "    #Version1\n",
    "    Art[x] = clean(Art[x])\n",
    "    Economy[x] = clean(Economy[x])\n",
    "    Health[x] = clean(Health[x])\n",
    "    Law[x] = clean(Law[x])\n",
    "    Literature[x] = clean(Literature[x])\n",
    "    Politics[x] = clean(Politics[x])\n",
    "    Religion[x] = clean(Religion[x])\n",
    "    Sport[x] = clean(Sport[x])\n",
    "    Technology[x] = clean(Technology[x])\n",
    "    \n",
    "    #Version2\n",
    "    Art2[x] = clean(Art2[x])\n",
    "    Economy2[x] = clean(Economy2[x])\n",
    "    Health2[x] = clean(Health2[x])\n",
    "    Law2[x] = clean(Law2[x])\n",
    "    Literature2[x] = clean(Literature2[x])\n",
    "    Politics2[x] = clean(Politics2[x])\n",
    "    Religion2[x] = clean(Religion2[x])\n",
    "    Sport2[x] = clean(Sport2[x])\n",
    "    Technology2[x] = clean(Technology2[x])\n",
    "\n",
    "    #Version3\n",
    "    Art3[x] = clean(Art3[x])\n",
    "    Economy3[x] = clean(Economy3[x])\n",
    "    Health3[x] = clean(Health3[x])\n",
    "    Law3[x] = clean(Law3[x])\n",
    "    Literature3[x] = clean(Literature3[x])\n",
    "    Politics3[x] = clean(Politics3[x])\n",
    "    Religion3[x] = clean(Religion3[x])\n",
    "    Sport3[x] = clean(Sport3[x])\n",
    "    Technology3[x] = clean(Technology3[x])\n",
    "    \n",
    "    #Version4\n",
    "    Art4[x] = clean(Art4[x])\n",
    "    Economy4[x] = clean(Economy4[x])\n",
    "    Health4[x] = clean(Health4[x])\n",
    "    Law4[x] = clean(Law4[x])\n",
    "    Literature4[x] = clean(Literature4[x])\n",
    "    Politics4[x] = clean(Politics4[x])\n",
    "    Religion4[x] = clean(Religion4[x])\n",
    "    Sport4[x] = clean(Sport4[x])\n",
    "    Technology4[x] = clean(Technology4[x])\n",
    "    \n",
    "    #Version5\n",
    "    Art5[x] = clean(Art5[x])\n",
    "    Economy5[x] = clean(Economy5[x])\n",
    "    Health5[x] = clean(Health5[x])\n",
    "    Law5[x] = clean(Law5[x])\n",
    "    Literature5[x] = clean(Literature5[x])\n",
    "    Politics5[x] = clean(Politics5[x])\n",
    "    Religion5[x] = clean(Religion5[x])\n",
    "    Sport5[x] = clean(Sport5[x])\n",
    "    Technology5[x] = clean(Technology5[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(300):\n",
    "    \n",
    "    #Version1\n",
    "    Art[x] = Art[x].replace(\"\\n\",\" \")\n",
    "    Art[x] = Art[x].replace(\"\\u200f\",\" \")\n",
    "    Economy[x] = Economy[x].replace(\"\\n\",\" \")\n",
    "    Economy[x] = Economy[x].replace(\"\\u200f\",\" \")\n",
    "    Health[x] = Health[x].replace(\"\\n\",\" \")\n",
    "    Health[x] = Health[x].replace(\"\\u200f\",\" \")\n",
    "    Law[x] = Law[x].replace(\"\\n\",\" \")\n",
    "    Law[x] = Law[x].replace(\"\\u200f\",\" \")\n",
    "    Literature[x] = Literature[x].replace(\"\\n\",\" \")\n",
    "    Literature[x] = Literature[x].replace(\"\\u200f\",\" \")\n",
    "    Politics[x] = Politics[x].replace(\"\\n\",\" \")\n",
    "    Politics[x] = Politics[x].replace(\"\\u200f\",\" \")\n",
    "    Religion[x] = Religion[x].replace(\"\\n\",\" \")\n",
    "    Religion[x] = Religion[x].replace(\"\\u200f\",\" \")\n",
    "    Sport[x] = Sport[x].replace(\"\\n\",\" \")\n",
    "    Sport[x] = Sport[x].replace(\"\\u200f\",\" \")\n",
    "    Technology[x] = Technology[x].replace(\"\\n\",\" \")\n",
    "    Technology[x] = Technology[x].replace(\"\\u200f\",\" \")\n",
    "    \n",
    "    #Version2\n",
    "    Art2[x] = Art2[x].replace(\"\\n\",\" \")\n",
    "    Art2[x] = Art2[x].replace(\"\\u200f\",\" \")\n",
    "    Economy2[x] = Economy2[x].replace(\"\\n\",\" \")\n",
    "    Economy2[x] = Economy2[x].replace(\"\\u200f\",\" \")\n",
    "    Health2[x] = Health2[x].replace(\"\\n\",\" \")\n",
    "    Health2[x] = Health2[x].replace(\"\\u200f\",\" \")\n",
    "    Law2[x] = Law2[x].replace(\"\\n\",\" \")\n",
    "    Law2[x] = Law2[x].replace(\"\\u200f\",\" \")\n",
    "    Literature2[x] = Literature2[x].replace(\"\\n\",\" \")\n",
    "    Literature2[x] = Literature2[x].replace(\"\\u200f\",\" \")\n",
    "    Politics2[x] = Politics2[x].replace(\"\\n\",\" \")\n",
    "    Politics2[x] = Politics2[x].replace(\"\\u200f\",\" \")\n",
    "    Religion2[x] = Religion2[x].replace(\"\\n\",\" \")\n",
    "    Religion2[x] = Religion2[x].replace(\"\\u200f\",\" \")\n",
    "    Sport2[x] = Sport2[x].replace(\"\\n\",\" \")\n",
    "    Sport2[x] = Sport2[x].replace(\"\\u200f\",\" \")\n",
    "    Technology2[x] = Technology2[x].replace(\"\\n\",\" \")\n",
    "    Technology2[x] = Technology2[x].replace(\"\\u200f\",\" \")\n",
    "\n",
    "    #Version3\n",
    "    Art3[x] = Art3[x].replace(\"\\n\",\" \")\n",
    "    Art3[x] = Art3[x].replace(\"\\u200f\",\" \")\n",
    "    Economy3[x] = Economy3[x].replace(\"\\n\",\" \")\n",
    "    Economy3[x] = Economy3[x].replace(\"\\u200f\",\" \")\n",
    "    Health3[x] = Health3[x].replace(\"\\n\",\" \")\n",
    "    Health3[x] = Health3[x].replace(\"\\u200f\",\" \")\n",
    "    Law3[x] = Law3[x].replace(\"\\n\",\" \")\n",
    "    Law3[x] = Law3[x].replace(\"\\u200f\",\" \")\n",
    "    Literature3[x] = Literature3[x].replace(\"\\n\",\" \")\n",
    "    Literature3[x] = Literature3[x].replace(\"\\u200f\",\" \")\n",
    "    Politics3[x] = Politics3[x].replace(\"\\n\",\" \")\n",
    "    Politics3[x] = Politics3[x].replace(\"\\u200f\",\" \")\n",
    "    Religion3[x] = Religion3[x].replace(\"\\n\",\" \")\n",
    "    Religion3[x] = Religion3[x].replace(\"\\u200f\",\" \")\n",
    "    Sport3[x] = Sport3[x].replace(\"\\n\",\" \")\n",
    "    Sport3[x] = Sport3[x].replace(\"\\u200f\",\" \")\n",
    "    Technology3[x] = Technology3[x].replace(\"\\n\",\" \")\n",
    "    Technology3[x] = Technology3[x].replace(\"\\u200f\",\" \")\n",
    "\n",
    "    #Version4\n",
    "    Art4[x] = Art4[x].replace(\"\\n\",\" \")\n",
    "    Art4[x] = Art4[x].replace(\"\\u200f\",\" \")\n",
    "    Economy4[x] = Economy4[x].replace(\"\\n\",\" \")\n",
    "    Economy4[x] = Economy4[x].replace(\"\\u200f\",\" \")\n",
    "    Health4[x] = Health4[x].replace(\"\\n\",\" \")\n",
    "    Health4[x] = Health4[x].replace(\"\\u200f\",\" \")\n",
    "    Law4[x] = Law4[x].replace(\"\\n\",\" \")\n",
    "    Law4[x] = Law4[x].replace(\"\\u200f\",\" \")\n",
    "    Literature4[x] = Literature4[x].replace(\"\\n\",\" \")\n",
    "    Literature4[x] = Literature4[x].replace(\"\\u200f\",\" \")\n",
    "    Politics4[x] = Politics4[x].replace(\"\\n\",\" \")\n",
    "    Politics4[x] = Politics4[x].replace(\"\\u200f\",\" \")\n",
    "    Religion4[x] = Religion4[x].replace(\"\\n\",\" \")\n",
    "    Religion4[x] = Religion4[x].replace(\"\\u200f\",\" \")\n",
    "    Sport4[x] = Sport4[x].replace(\"\\n\",\" \")\n",
    "    Sport4[x] = Sport4[x].replace(\"\\u200f\",\" \")\n",
    "    Technology4[x] = Technology4[x].replace(\"\\n\",\" \")\n",
    "    Technology4[x] = Technology4[x].replace(\"\\u200f\",\" \")\n",
    "\n",
    "    #Version5\n",
    "    Art5[x] = Art5[x].replace(\"\\n\",\" \")\n",
    "    Art5[x] = Art5[x].replace(\"\\u200f\",\" \")\n",
    "    Economy5[x] = Economy5[x].replace(\"\\n\",\" \")\n",
    "    Economy5[x] = Economy5[x].replace(\"\\u200f\",\" \")\n",
    "    Health5[x] = Health5[x].replace(\"\\n\",\" \")\n",
    "    Health5[x] = Health5[x].replace(\"\\u200f\",\" \")\n",
    "    Law5[x] = Law5[x].replace(\"\\n\",\" \")\n",
    "    Law5[x] = Law5[x].replace(\"\\u200f\",\" \")\n",
    "    Literature5[x] = Literature5[x].replace(\"\\n\",\" \")\n",
    "    Literature5[x] = Literature5[x].replace(\"\\u200f\",\" \")\n",
    "    Politics5[x] = Politics5[x].replace(\"\\n\",\" \")\n",
    "    Politics5[x] = Politics5[x].replace(\"\\u200f\",\" \")\n",
    "    Religion5[x] = Religion5[x].replace(\"\\n\",\" \")\n",
    "    Religion5[x] = Religion5[x].replace(\"\\u200f\",\" \")\n",
    "    Sport5[x] = Sport5[x].replace(\"\\n\",\" \")\n",
    "    Sport5[x] = Sport5[x].replace(\"\\u200f\",\" \")\n",
    "    Technology5[x] = Technology5[x].replace(\"\\n\",\" \")\n",
    "    Technology5[x] = Technology5[x].replace(\"\\u200f\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we did this for each version\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Art_v5.txt\",Art5)\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Economy_v5.txt\",Economy5)\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Health_v5.txt\",Health5)\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Law_v5.txt\",Law5)\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Literature_v5.txt\",Literature5)\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Politics_v5.txt\",Politics5)\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Religion_v5.txt\",Religion5)\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Sport_v5.txt\",Sport5)\n",
    "#WriteToFile(\"/Users/keemo/Desktop/Phase1/joined_dataset/version5/Technology_v5.txt\",Technology5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data after preprocssing using madamira inside variables\n",
    "\n",
    "#Version1\n",
    "ArtPOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Art_v1.txt.bpc\")\n",
    "EconomyPOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Economy_v1.txt.bpc\")\n",
    "HealthPOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Health_v1.txt.bpc\")\n",
    "LawPOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Law_v1.txt.bpc\")\n",
    "LiteraturePOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Literature_v1.txt.bpc\")\n",
    "PoliticsPOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Politics_v1.txt.bpc\")\n",
    "ReligionPOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Religion_v1.txt.bpc\")\n",
    "SportPOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Sport_v1.txt.bpc\")\n",
    "TechnologyPOS = ReadFromFile(\"../../Datasets/POS_joined_dataset/version1/Technology_v1.txt.bpc\")\n",
    "\n",
    "#Version2\n",
    "ArtPOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Art_v2.txt.bpc\")\n",
    "EconomyPOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Economy_v2.txt.bpc\")\n",
    "HealthPOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Health_v2.txt.bpc\")\n",
    "LawPOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Law_v2.txt.bpc\")\n",
    "LiteraturePOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Literature_v2.txt.bpc\")\n",
    "PoliticsPOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Politics_v2.txt.bpc\")\n",
    "ReligionPOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Religion_v2.txt.bpc\")\n",
    "SportPOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Sport_v2.txt.bpc\")\n",
    "TechnologyPOS2 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version2/Technology_v2.txt.bpc\")\n",
    "\n",
    "#Version3\n",
    "ArtPOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Art_v3.txt.bpc\")\n",
    "EconomyPOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Economy_v3.txt.bpc\")\n",
    "HealthPOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Health_v3.txt.bpc\")\n",
    "LawPOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Law_v3.txt.bpc\")\n",
    "LiteraturePOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Literature_v3.txt.bpc\")\n",
    "PoliticsPOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Politics_v3.txt.bpc\")\n",
    "ReligionPOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Religion_v3.txt.bpc\")\n",
    "SportPOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Sport_v3.txt.bpc\")\n",
    "TechnologyPOS3 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version3/Technology_v3.txt.bpc\")\n",
    "\n",
    "#Version4\n",
    "ArtPOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Art_v4.txt.bpc\")\n",
    "EconomyPOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Economy_v4.txt.bpc\")\n",
    "HealthPOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Health_v4.txt.bpc\")\n",
    "LawPOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Law_v4.txt.bpc\")\n",
    "LiteraturePOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Literature_v4.txt.bpc\")\n",
    "PoliticsPOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Politics_v4.txt.bpc\")\n",
    "ReligionPOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Religion_v4.txt.bpc\")\n",
    "SportPOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Sport_v4.txt.bpc\")\n",
    "TechnologyPOS4 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version4/Technology_v4.txt.bpc\")\n",
    "\n",
    "#Version5\n",
    "ArtPOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Art_v5.txt.bpc\")\n",
    "EconomyPOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Economy_v5.txt.bpc\")\n",
    "HealthPOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Health_v5.txt.bpc\")\n",
    "LawPOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Law_v5.txt.bpc\")\n",
    "LiteraturePOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Literature_v5.txt.bpc\")\n",
    "PoliticsPOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Politics_v5.txt.bpc\")\n",
    "ReligionPOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Religion_v5.txt.bpc\")\n",
    "SportPOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Sport_v5.txt.bpc\")\n",
    "TechnologyPOS5 = ReadFromFile(\"../../Datasets/POS_joined_dataset/version5/Technology_v5.txt.bpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we did this for each version to obtain 5 different dataframes for each version\n",
    "\n",
    "#Version1\n",
    "dfnames1 = ['version1_df1','version1_df2','version1_df3','version1_df4','version1_df5','version1_df6',\n",
    "           'version1_df7','version1_df8','version1_df9']\n",
    "for x in dfnames1: exec(x + ' = pd.DataFrame()')    \n",
    "\n",
    "version1_df1['text'], version1_df1['topic'] = [ArtPOS, 'Art']\n",
    "version1_df2['text'], version1_df2['topic'] = [EconomyPOS, 'Economy']\n",
    "version1_df3['text'], version1_df3['topic'] = [HealthPOS, 'Health']\n",
    "version1_df4['text'], version1_df4['topic'] = [LawPOS, 'Law']\n",
    "version1_df5['text'], version1_df5['topic'] = [LiteraturePOS, 'Literature']\n",
    "version1_df6['text'], version1_df6['topic'] = [PoliticsPOS, 'Politics']\n",
    "version1_df7['text'], version1_df7['topic'] = [ReligionPOS, 'Religion']\n",
    "version1_df8['text'], version1_df8['topic'] = [SportPOS, 'Sport']\n",
    "version1_df9['text'], version1_df9['topic'] = [TechnologyPOS, 'Technology']\n",
    "\n",
    "data_frames1 = [version1_df1, version1_df2, version1_df3,version1_df4,version1_df5,version1_df6,version1_df7,\n",
    "               version1_df8,version1_df9]\n",
    "version1_Data = pd.concat(data_frames1,ignore_index = True)\n",
    "\n",
    "#Version2\n",
    "dfnames2 = ['version2_df1','version2_df2','version2_df3','version2_df4','version2_df5','version2_df6',\n",
    "           'version2_df7','version2_df8','version2_df9']\n",
    "for x in dfnames2: exec(x + ' = pd.DataFrame()')    \n",
    "\n",
    "version2_df1['text'], version2_df1['topic'] = [ArtPOS2, 'Art']\n",
    "version2_df2['text'], version2_df2['topic'] = [EconomyPOS2, 'Economy']\n",
    "version2_df3['text'], version2_df3['topic'] = [HealthPOS2, 'Health']\n",
    "version2_df4['text'], version2_df4['topic'] = [LawPOS2, 'Law']\n",
    "version2_df5['text'], version2_df5['topic'] = [LiteraturePOS2, 'Literature']\n",
    "version2_df6['text'], version2_df6['topic'] = [PoliticsPOS2, 'Politics']\n",
    "version2_df7['text'], version2_df7['topic'] = [ReligionPOS2, 'Religion']\n",
    "version2_df8['text'], version2_df8['topic'] = [SportPOS2, 'Sport']\n",
    "version2_df9['text'], version2_df9['topic'] = [TechnologyPOS2, 'Technology']\n",
    "\n",
    "data_frames2 = [version2_df1, version2_df2, version2_df3,version2_df4,version2_df5,version2_df6,version2_df7,\n",
    "               version2_df8,version2_df9]\n",
    "version2_Data = pd.concat(data_frames2,ignore_index = True)\n",
    "\n",
    "#Version3\n",
    "dfnames3 = ['version3_df1','version3_df2','version3_df3','version3_df4','version3_df5','version3_df6',\n",
    "           'version3_df7','version3_df8','version3_df9']\n",
    "for x in dfnames3: exec(x + ' = pd.DataFrame()')    \n",
    "\n",
    "version3_df1['text'], version3_df1['topic'] = [ArtPOS3, 'Art']\n",
    "version3_df2['text'], version3_df2['topic'] = [EconomyPOS3, 'Economy']\n",
    "version3_df3['text'], version3_df3['topic'] = [HealthPOS3, 'Health']\n",
    "version3_df4['text'], version3_df4['topic'] = [LawPOS3, 'Law']\n",
    "version3_df5['text'], version3_df5['topic'] = [LiteraturePOS3, 'Literature']\n",
    "version3_df6['text'], version3_df6['topic'] = [PoliticsPOS3, 'Politics']\n",
    "version3_df7['text'], version3_df7['topic'] = [ReligionPOS3, 'Religion']\n",
    "version3_df8['text'], version3_df8['topic'] = [SportPOS3, 'Sport']\n",
    "version3_df9['text'], version3_df9['topic'] = [TechnologyPOS3, 'Technology']\n",
    "\n",
    "data_frames3 = [version3_df1, version3_df2, version3_df3,version3_df4,version3_df5,version3_df6,version3_df7,\n",
    "               version3_df8,version3_df9]\n",
    "version3_Data = pd.concat(data_frames3,ignore_index = True)\n",
    "\n",
    "#Version4\n",
    "dfnames4 = ['version4_df1','version4_df2','version4_df3','version4_df4','version4_df5','version4_df6',\n",
    "           'version4_df7','version4_df8','version4_df9']\n",
    "for x in dfnames4: exec(x + ' = pd.DataFrame()')    \n",
    "\n",
    "version4_df1['text'], version4_df1['topic'] = [ArtPOS4, 'Art']\n",
    "version4_df2['text'], version4_df2['topic'] = [EconomyPOS4, 'Economy']\n",
    "version4_df3['text'], version4_df3['topic'] = [HealthPOS4, 'Health']\n",
    "version4_df4['text'], version4_df4['topic'] = [LawPOS4, 'Law']\n",
    "version4_df5['text'], version4_df5['topic'] = [LiteraturePOS4, 'Literature']\n",
    "version4_df6['text'], version4_df6['topic'] = [PoliticsPOS4, 'Politics']\n",
    "version4_df7['text'], version4_df7['topic'] = [ReligionPOS4, 'Religion']\n",
    "version4_df8['text'], version4_df8['topic'] = [SportPOS4, 'Sport']\n",
    "version4_df9['text'], version4_df9['topic'] = [TechnologyPOS4, 'Technology']\n",
    "\n",
    "data_frames4 = [version4_df1, version4_df2, version4_df3,version4_df4,version4_df5,version4_df6,version4_df7,\n",
    "               version4_df8,version4_df9]\n",
    "version4_Data = pd.concat(data_frames4,ignore_index = True)\n",
    "\n",
    "#Version5\n",
    "dfnames5 = ['version5_df1','version5_df2','version5_df3','version5_df4','version5_df5','version5_df6',\n",
    "           'version5_df7','version5_df8','version5_df9']\n",
    "for x in dfnames5: exec(x + ' = pd.DataFrame()')    \n",
    "\n",
    "version5_df1['text'], version5_df1['topic'] = [ArtPOS5, 'Art']\n",
    "version5_df2['text'], version5_df2['topic'] = [EconomyPOS5, 'Economy']\n",
    "version5_df3['text'], version5_df3['topic'] = [HealthPOS5, 'Health']\n",
    "version5_df4['text'], version5_df4['topic'] = [LawPOS5, 'Law']\n",
    "version5_df5['text'], version5_df5['topic'] = [LiteraturePOS5, 'Literature']\n",
    "version5_df6['text'], version5_df6['topic'] = [PoliticsPOS5, 'Politics']\n",
    "version5_df7['text'], version5_df7['topic'] = [ReligionPOS5, 'Religion']\n",
    "version5_df8['text'], version5_df8['topic'] = [SportPOS5, 'Sport']\n",
    "version5_df9['text'], version5_df9['topic'] = [TechnologyPOS5, 'Technology']\n",
    "\n",
    "data_frames5 = [version5_df1, version5_df2, version5_df3,version5_df4,version5_df5,version5_df6,version5_df7,\n",
    "               version5_df8,version5_df9]\n",
    "version5_Data = pd.concat(data_frames5,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[VP يبدو ] [SBAR ان ] [NP سوء ] [NP ال+ طالع ...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[VP عادت ] [NP ال+ فنانة ] [NP ال+ شابة ميس ح...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[NP صرح ] [NP ال+ فنان احمد ] [ADJP زاهر ] [P...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[PP منذ ] [SBAR ان ] [VP تم ] [NP اسناد ] [NP...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[VP تسود ] [NP حال ] [NP +ه ] [PP من ] [NP ال...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text topic\n",
       "0   [VP يبدو ] [SBAR ان ] [NP سوء ] [NP ال+ طالع ...   Art\n",
       "1   [VP عادت ] [NP ال+ فنانة ] [NP ال+ شابة ميس ح...   Art\n",
       "2   [NP صرح ] [NP ال+ فنان احمد ] [ADJP زاهر ] [P...   Art\n",
       "3   [PP منذ ] [SBAR ان ] [VP تم ] [NP اسناد ] [NP...   Art\n",
       "4   [VP تسود ] [NP حال ] [NP +ه ] [PP من ] [NP ال...   Art"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version1_Data.drop(version1_Data.tail(1).index, inplace = True) \n",
    "version1_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[VP يبدو ] [NP سوء ] [NP ال+ طالع ] [VP يرافق...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[VP عادت ] [NP ال+ فنانة ] [NP ال+ شابة ميس ح...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[NP صرح ] [NP ال+ فنان احمد ] [ADJP زاهر ] [P...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[VP تم ] [NP اسناد ] [NP بطولة ] [NP ال+ فيلم...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[VP تسود ] [NP حال ] [NP +ه ] [NP ال+ غموض ] ...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text topic\n",
       "0   [VP يبدو ] [NP سوء ] [NP ال+ طالع ] [VP يرافق...   Art\n",
       "1   [VP عادت ] [NP ال+ فنانة ] [NP ال+ شابة ميس ح...   Art\n",
       "2   [NP صرح ] [NP ال+ فنان احمد ] [ADJP زاهر ] [P...   Art\n",
       "3   [VP تم ] [NP اسناد ] [NP بطولة ] [NP ال+ فيلم...   Art\n",
       "4   [VP تسود ] [NP حال ] [NP +ه ] [NP ال+ غموض ] ...   Art"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version2_Data.drop(version2_Data.tail(1).index, inplace = True) \n",
    "version2_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[VP يبدو ] [NP سوء طالع ] [VP يرافق ] [NP فن ...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[VP عادت ] [NP فنان ] [NP شاب ميس حمد ] [ADJP...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[NP صرح ] [NP فن ] [NP احمد ] [ADJP زاهر ] [P...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[VP تم ] [NP اسناد ] [PP ب+ ] [NP طول ] [NP ف...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[VP تسود ] [NP حال ] [NP غموض ] [VP صفق ] [NP...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text topic\n",
       "0   [VP يبدو ] [NP سوء طالع ] [VP يرافق ] [NP فن ...   Art\n",
       "1   [VP عادت ] [NP فنان ] [NP شاب ميس حمد ] [ADJP...   Art\n",
       "2   [NP صرح ] [NP فن ] [NP احمد ] [ADJP زاهر ] [P...   Art\n",
       "3   [VP تم ] [NP اسناد ] [PP ب+ ] [NP طول ] [NP ف...   Art\n",
       "4   [VP تسود ] [NP حال ] [NP غموض ] [VP صفق ] [NP...   Art"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version3_Data.drop(version3_Data.tail(1).index, inplace = True) \n",
    "version3_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[VP يبد ] [NP سوء طالع ] [VP يرافق ] [NP فن ]...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[NP عاد ] [NP فنان ] [NP شاب ميس حمد ] [ADJP ...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[NP صرح ] [NP فن ] [NP احمد ] [ADJP زاهر ] [S...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[VP تم ] [NP اسناد ] [NP طول ] [NP فيلم ] [NP...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[VP تسود ] [NP حال ] [NP غموض ] [VP صفق ] [NP...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text topic\n",
       "0   [VP يبد ] [NP سوء طالع ] [VP يرافق ] [NP فن ]...   Art\n",
       "1   [NP عاد ] [NP فنان ] [NP شاب ميس حمد ] [ADJP ...   Art\n",
       "2   [NP صرح ] [NP فن ] [NP احمد ] [ADJP زاهر ] [S...   Art\n",
       "3   [VP تم ] [NP اسناد ] [NP طول ] [NP فيلم ] [NP...   Art\n",
       "4   [VP تسود ] [NP حال ] [NP غموض ] [VP صفق ] [NP...   Art"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version4_Data.drop(version4_Data.tail(1).index, inplace = True) \n",
    "version4_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[VP بدا ] [NP سوا ] [VP طلع ] [NP رفق ] [NP ف...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[NP عود ] [NP فنن شوب ميس حمد ] [NP قهر ] [NP...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[NP صرح ] [NP فنن ] [NP حمد ] [NP زهر ] [PP ب...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[VP يتم ] [NP سند ] [NP بطل ] [NP فيلم ] [NP ...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NP سود ] [NP حول ] [NP غمض ] [VP صفق ] [NP ش...</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text topic\n",
       "0   [VP بدا ] [NP سوا ] [VP طلع ] [NP رفق ] [NP ف...   Art\n",
       "1   [NP عود ] [NP فنن شوب ميس حمد ] [NP قهر ] [NP...   Art\n",
       "2   [NP صرح ] [NP فنن ] [NP حمد ] [NP زهر ] [PP ب...   Art\n",
       "3   [VP يتم ] [NP سند ] [NP بطل ] [NP فيلم ] [NP ...   Art\n",
       "4   [NP سود ] [NP حول ] [NP غمض ] [VP صفق ] [NP ش...   Art"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version5_Data.drop(version5_Data.tail(1).index, inplace = True) \n",
    "version5_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>[NP قوم ] [VP لكسمارك ] [NP علم ] [NP كبر ] و...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>[VP بدا ] [NP طقم ] [VP حوط ] [NP فض ] [NP +ي...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>[PP عدا ] [VP هتف ] [NP حدث حلل ] [VP قدم ] [...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>[VP جمع ] [NP حمس ] [VP تجر ] [NP شر ] [NP +ك...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>[NP علن ] [NP علم ] [NP كور ] [NP جنب ] [NP ط...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       topic\n",
       "2703   [NP قوم ] [VP لكسمارك ] [NP علم ] [NP كبر ] و...  Technology\n",
       "2704   [VP بدا ] [NP طقم ] [VP حوط ] [NP فض ] [NP +ي...  Technology\n",
       "2705   [PP عدا ] [VP هتف ] [NP حدث حلل ] [VP قدم ] [...  Technology\n",
       "2706   [VP جمع ] [NP حمس ] [VP تجر ] [NP شر ] [NP +ك...  Technology\n",
       "2707   [NP علن ] [NP علم ] [NP كور ] [NP جنب ] [NP ط...  Technology"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version5_Data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Art\n",
      "1    Art\n",
      "2    Art\n",
      "3    Art\n",
      "4    Art\n",
      "Name: topic, dtype: object\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2703    8\n",
      "2704    8\n",
      "2705    8\n",
      "2706    8\n",
      "2707    8\n",
      "Name: topic, Length: 2708, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# extract labels to fit them  \n",
    "# version1\n",
    "label1 = version1_Data['topic']\n",
    "# version2\n",
    "label2 = version2_Data['topic']\n",
    "# version3\n",
    "label3 = version3_Data['topic']\n",
    "# version4\n",
    "label4 = version4_Data['topic']\n",
    "# version5\n",
    "label5 = version5_Data['topic']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "label_map = {\n",
    "    'Art' : 0,\n",
    "    'Economy' : 1,\n",
    "    'Health' : 2,\n",
    "    'Law' : 3,\n",
    "    'Literature':4,\n",
    "    'Politics':5,\n",
    "    'Religion':6,\n",
    "    'Sport':7,\n",
    "    'Technology':8}\n",
    "\n",
    "\n",
    "\n",
    "y_enc1 = label1.apply(lambda x: label_map[x])\n",
    "y_enc2 = label2.apply(lambda x: label_map[x])\n",
    "y_enc3 = label3.apply(lambda x: label_map[x])\n",
    "y_enc4 = label4.apply(lambda x: label_map[x])\n",
    "y_enc5 = label5.apply(lambda x: label_map[x])\n",
    "\n",
    "\n",
    "print(label1.head())\n",
    "print(y_enc1)\n",
    "#==\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "# create train and test data\n",
    "\n",
    "# version1\n",
    "X_train1,X_test1,y_train1,y_test1 = split(version1_Data['text'],y_enc1, test_size = 0.2, random_state = 10)\n",
    "vect.fit(X_train1)\n",
    "X_train1_df = vect.transform(X_train1)\n",
    "X_test1_df = vect.transform(X_test1)\n",
    "\n",
    "# version2\n",
    "X_train2,X_test2,y_train2,y_test2 = split(version2_Data['text'],y_enc2, test_size = 0.2, random_state = 10)\n",
    "vect.fit(X_train2)\n",
    "X_train2_df = vect.transform(X_train2)\n",
    "X_test2_df = vect.transform(X_test2)\n",
    "\n",
    "# version3\n",
    "X_train3,X_test3,y_train3,y_test3 = split(version3_Data['text'],y_enc3, test_size = 0.2, random_state = 10)\n",
    "vect.fit(X_train3)\n",
    "X_train3_df = vect.transform(X_train3)\n",
    "X_test3_df = vect.transform(X_test3)\n",
    "\n",
    "# version4\n",
    "X_train4,X_test4,y_train4,y_test4 = split(version4_Data['text'],y_enc4, test_size = 0.2, random_state = 10)\n",
    "vect.fit(X_train4)\n",
    "X_train4_df = vect.transform(X_train4)\n",
    "X_test4_df = vect.transform(X_test4)\n",
    "\n",
    "# version5\n",
    "X_train5,X_test5,y_train5,y_test5 = split(version5_Data['text'],y_enc5, test_size = 0.2, random_state = 10)\n",
    "vect.fit(X_train5)\n",
    "X_train5_df = vect.transform(X_train5)\n",
    "X_test5_df = vect.transform(X_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version1_Data.to_csv(\"version1.csv\",encoding=\"utf-16\")\n",
    "#version2_Data.to_csv(\"version2.csv\",encoding=\"utf-16\")\n",
    "#version3_Data.to_csv(\"version3.csv\",encoding=\"utf-16\")\n",
    "#version4_Data.to_csv(\"version4.csv\",encoding=\"utf-16\")\n",
    "#version5_Data.to_csv(\"version5.csv\",encoding=\"utf-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to save predictions \n",
    "prediction = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 8 0 5 4 5 6 5 3 8 8 5 2 3 7 2 2 6 3 8 4 0 8 5 4 2 2 8 1 7 2 7 4 3 3 2 8\n",
      " 8 5 8 7 7 6 0 2 1 3 8 1 8 2 4 6 6 8 1 1 6 4 1 3 3 5 8 8 1 4 2 4 6 6 0 6 2\n",
      " 2 5 5 2 1 7 6 6 0 2 7 0 2 3 5 8 5 4 8 2 1 4 8 3 0 7 4 5 0 3 0 8 8 7 6 8 1\n",
      " 0 7 2 4 2 1 7 0 2 2 0 8 3 7 0 5 2 4 7 5 3 7 4 1 8 0 5 7 7 7 8 8 0 7 1 2 2\n",
      " 8 2 1 7 4 0 0 8 3 0 3 4 6 5 4 1 6 4 6 6 3 5 8 4 5 3 7 6 3 5 3 2 1 0 3 2 4\n",
      " 5 6 7 1 0 0 7 3 8 7 5 2 1 6 4 8 3 7 0 8 6 8 3 1 5 1 0 7 3 7 4 0 5 7 3 7 4\n",
      " 4 6 4 4 3 8 5 6 1 7 3 0 2 4 2 8 0 5 6 0 3 4 6 7 2 7 2 3 7 4 2 5 4 7 4 0 3\n",
      " 2 2 0 5 7 2 1 0 0 5 7 3 3 3 3 1 3 0 7 4 1 7 2 7 0 0 0 2 1 3 0 7 2 0 6 3 4\n",
      " 0 0 5 3 8 5 2 3 8 0 7 5 5 0 5 6 0 7 2 5 5 6 6 2 1 2 4 4 0 7 2 7 2 6 0 4 0\n",
      " 0 7 2 6 6 1 8 7 4 8 4 6 0 4 2 8 7 6 6 6 0 3 4 1 6 0 6 2 7 2 7 6 5 6 0 7 2\n",
      " 3 5 7 6 0 5 5 0 3 5 3 6 2 3 2 8 5 7 0 5 4 1 1 4 8 0 7 5 3 8 0 5 2 3 1 2 4\n",
      " 8 3 5 4 8 2 1 4 4 7 0 2 1 7 2 7 8 5 7 3 2 6 6 5 5 7 5 2 7 4 7 3 6 2 8 3 1\n",
      " 4 5 8 5 3 1 4 7 6 4 7 7 5 6 2 7 7 3 4 8 0 3 1 8 7 5 3 5 6 3 0 7 4 6 2 7 3\n",
      " 7 0 1 2 3 6 2 2 4 2 2 2 6 3 6 2 5 0 6 2 8 4 6 5 2 3 2 0 8 7 6 5 8 4 2 3 1\n",
      " 2 3 2 5 8 6 0 8 8 0 1 2 7 6 1 1 4 0 0 8 5 2 0 5]\n"
     ]
    }
   ],
   "source": [
    "## bulid a model using support victor machine then predict test data\n",
    "# version1\n",
    "svm1 = LinearSVC().fit(X_train1_df,y_train1)\n",
    "prediction[\"svm1\"] = svm1.predict(X_test1_df)\n",
    "# version2\n",
    "svm2 = LinearSVC().fit(X_train2_df,y_train2)\n",
    "prediction[\"svm2\"] = svm2.predict(X_test2_df)\n",
    "# version3\n",
    "svm3 = LinearSVC().fit(X_train3_df,y_train3)\n",
    "prediction[\"svm3\"] = svm3.predict(X_test3_df)\n",
    "# version4\n",
    "svm4 = LinearSVC().fit(X_train4_df,y_train4)\n",
    "prediction[\"svm4\"] = svm4.predict(X_test4_df)\n",
    "# version5\n",
    "svm5 = LinearSVC().fit(X_train5_df,y_train5)\n",
    "prediction[\"svm5\"] = svm5.predict(X_test5_df)\n",
    "\n",
    "print(prediction[\"svm5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 8 0 5 4 5 5 0 3 8 8 5 2 3 7 2 1 0 3 4 4 4 8 6 4 2 2 8 1 7 2 7 6 5 3 2 5\n",
      " 8 3 8 7 7 6 0 2 1 3 8 1 8 8 4 6 6 8 1 3 6 2 5 1 7 5 8 8 1 6 2 4 6 6 0 6 2\n",
      " 2 5 3 6 1 7 4 4 0 2 7 0 4 6 5 8 5 4 8 5 1 4 8 5 0 7 4 5 0 3 0 8 8 7 6 8 1\n",
      " 0 7 2 4 2 1 7 0 2 2 0 8 3 7 0 5 2 0 7 5 5 7 6 6 8 0 5 7 7 7 6 8 0 7 5 2 2\n",
      " 8 2 5 7 4 0 0 8 5 6 6 6 6 5 4 1 6 4 6 6 5 5 8 4 5 3 8 3 5 8 3 2 1 0 3 6 2\n",
      " 5 6 7 5 0 0 7 3 8 7 5 6 1 6 4 8 3 7 0 8 6 8 3 6 5 1 0 7 6 7 4 0 5 7 5 4 4\n",
      " 4 6 4 4 3 8 5 6 1 7 5 0 2 4 2 8 0 4 6 0 6 4 6 7 4 7 2 3 7 3 2 5 3 6 4 0 0\n",
      " 8 2 0 5 7 2 6 0 6 5 7 3 6 6 3 1 3 0 7 4 1 7 2 7 6 6 4 2 1 6 6 7 2 4 6 6 0\n",
      " 0 0 5 3 8 5 2 6 8 0 3 5 5 0 5 5 4 7 8 5 6 6 3 2 1 2 4 0 0 7 2 7 2 6 0 3 0\n",
      " 0 7 2 6 6 6 5 6 4 8 5 6 5 0 2 8 7 3 6 6 0 5 4 6 6 0 6 1 7 6 7 6 5 6 0 7 2\n",
      " 3 5 7 6 0 5 5 0 3 4 5 6 2 3 2 8 6 7 4 5 4 1 1 4 8 0 7 5 6 8 0 5 2 4 4 2 6\n",
      " 8 6 5 4 8 2 1 0 4 7 0 2 8 7 2 7 8 5 7 4 8 6 4 0 5 5 3 2 7 4 7 5 4 2 8 1 1\n",
      " 1 5 1 5 3 5 6 7 6 0 0 7 5 6 2 7 7 3 6 8 0 6 1 8 6 3 1 5 6 5 4 7 6 6 2 7 3\n",
      " 7 0 1 2 3 5 4 2 4 2 2 2 6 4 6 2 5 0 6 2 8 0 6 5 2 3 2 0 8 7 6 5 8 4 2 1 1\n",
      " 2 5 2 5 8 6 0 4 8 0 4 2 7 6 1 1 4 0 0 8 5 5 0 5]\n"
     ]
    }
   ],
   "source": [
    "## bulid a model using Naive Bayes then predict test data\n",
    "# version1\n",
    "nb1 = GaussianNB().fit(X_train1_df.todense(),y_train1)\n",
    "prediction[\"nb1\"] = nb1.predict(X_test1_df.todense())\n",
    "# version2\n",
    "nb2 = GaussianNB().fit(X_train2_df.todense(),y_train2)\n",
    "prediction[\"nb2\"] = nb2.predict(X_test2_df.todense())\n",
    "# version3\n",
    "nb3 = GaussianNB().fit(X_train3_df.todense(),y_train3)\n",
    "prediction[\"nb3\"] = nb3.predict(X_test3_df.todense())\n",
    "# version4\n",
    "nb4 = GaussianNB().fit(X_train4_df.todense(),y_train4)\n",
    "prediction[\"nb4\"] = nb4.predict(X_test4_df.todense())\n",
    "# version5\n",
    "nb5 = GaussianNB().fit(X_train5_df.todense(),y_train5)\n",
    "prediction[\"nb5\"] = nb5.predict(X_test5_df.todense())\n",
    "\n",
    "\n",
    "print(prediction[\"nb5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 2 4 5 3 5 3 1 8 5 2 3 7 2 0 5 5 8 4 0 8 5 4 2 2 8 1 7 2 7 4 8 3 1 0\n",
      " 8 5 8 7 7 6 0 2 1 2 7 1 8 2 4 6 0 8 1 1 2 4 8 3 3 5 8 8 1 2 2 6 6 6 0 6 2\n",
      " 2 8 1 0 1 7 2 6 0 2 7 0 2 3 4 8 5 0 8 2 1 4 0 4 0 7 4 7 0 3 0 6 8 7 2 8 1\n",
      " 4 8 6 4 2 1 7 8 2 1 3 4 3 7 0 3 2 1 7 3 3 7 4 1 8 0 2 7 0 7 8 1 0 7 0 2 2\n",
      " 8 8 4 7 4 0 0 8 3 0 3 0 6 5 4 4 6 4 6 6 5 5 8 3 5 1 7 6 6 3 2 2 1 2 3 0 4\n",
      " 5 6 7 1 0 0 7 5 8 7 5 2 6 5 4 8 3 7 0 8 3 8 4 1 5 1 0 7 3 7 4 4 5 7 3 7 4\n",
      " 6 2 4 0 3 8 5 4 1 7 3 2 2 4 2 8 0 5 8 2 8 4 6 7 2 7 2 3 7 6 2 3 4 7 0 0 3\n",
      " 2 2 0 5 1 2 1 0 0 5 7 3 3 3 3 1 3 0 7 4 1 7 2 7 0 0 0 2 1 3 0 7 2 0 6 3 4\n",
      " 3 3 8 3 8 4 2 5 8 0 7 5 5 0 5 8 0 7 2 5 5 8 6 2 1 8 4 4 0 7 2 7 7 6 0 4 7\n",
      " 0 7 2 6 6 1 8 7 4 8 4 6 6 5 1 1 7 6 6 6 0 3 4 1 6 4 6 0 7 2 7 4 1 6 0 7 2\n",
      " 3 5 7 6 0 5 5 0 3 3 3 6 2 3 7 3 5 7 0 5 4 8 2 0 8 0 7 5 8 8 0 5 2 5 1 2 4\n",
      " 4 3 5 4 8 2 1 4 4 8 0 8 8 7 2 7 8 3 7 3 8 6 6 1 3 7 8 2 7 4 7 3 5 2 1 8 1\n",
      " 4 5 3 5 3 1 4 7 6 5 1 7 5 6 2 7 7 3 4 8 0 3 1 8 7 5 3 8 6 3 0 7 4 6 2 7 3\n",
      " 7 8 1 2 3 6 2 2 4 5 1 8 6 3 6 2 3 7 6 1 8 4 3 5 2 3 2 0 8 7 4 4 8 4 2 3 1\n",
      " 1 3 2 5 8 6 0 8 8 3 5 2 5 8 1 1 4 0 0 8 6 8 0 5]\n"
     ]
    }
   ],
   "source": [
    "## bulid a model using Decision Tree then predict test data\n",
    "# version1\n",
    "dt1 = DecisionTreeClassifier().fit(X_train1_df,y_train1)\n",
    "prediction[\"dt1\"] = dt1.predict(X_test1_df)\n",
    "# version2\n",
    "dt2 = DecisionTreeClassifier().fit(X_train2_df,y_train2)\n",
    "prediction[\"dt2\"] = dt2.predict(X_test2_df)\n",
    "# version3\n",
    "dt3 = DecisionTreeClassifier().fit(X_train3_df,y_train3)\n",
    "prediction[\"dt3\"] = dt3.predict(X_test3_df)\n",
    "# version4\n",
    "dt4 = DecisionTreeClassifier().fit(X_train4_df,y_train4)\n",
    "prediction[\"dt4\"] = dt4.predict(X_test4_df)\n",
    "# version5\n",
    "dt5 = DecisionTreeClassifier().fit(X_train5_df,y_train5)\n",
    "prediction[\"dt5\"] = dt5.predict(X_test5_df)\n",
    "\n",
    "\n",
    "print(prediction[\"dt5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The confusion matrix for version 1 data using support victor machine\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        66\n",
      "           1       1.00      0.95      0.98        44\n",
      "           2       0.97      1.00      0.99        73\n",
      "           3       0.90      0.97      0.93        58\n",
      "           4       1.00      0.95      0.97        60\n",
      "           5       0.92      0.92      0.92        60\n",
      "           6       0.98      0.95      0.96        57\n",
      "           7       1.00      1.00      1.00        69\n",
      "           8       0.98      0.98      0.98        55\n",
      "\n",
      "    accuracy                           0.97       542\n",
      "   macro avg       0.97      0.97      0.97       542\n",
      "weighted avg       0.97      0.97      0.97       542\n",
      "\n",
      " The confusion matrix for version 2 data using support victor machine\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        66\n",
      "           1       1.00      0.95      0.98        44\n",
      "           2       0.97      1.00      0.99        73\n",
      "           3       0.90      0.97      0.93        58\n",
      "           4       1.00      0.95      0.97        60\n",
      "           5       0.92      0.92      0.92        60\n",
      "           6       0.98      0.95      0.96        57\n",
      "           7       1.00      1.00      1.00        69\n",
      "           8       0.98      0.98      0.98        55\n",
      "\n",
      "    accuracy                           0.97       542\n",
      "   macro avg       0.97      0.97      0.97       542\n",
      "weighted avg       0.97      0.97      0.97       542\n",
      "\n",
      " The confusion matrix for version 3 data using support victor machine\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        66\n",
      "           1       1.00      0.95      0.98        44\n",
      "           2       0.97      1.00      0.99        73\n",
      "           3       0.90      0.97      0.93        58\n",
      "           4       1.00      0.95      0.97        60\n",
      "           5       0.92      0.92      0.92        60\n",
      "           6       0.98      0.95      0.96        57\n",
      "           7       1.00      1.00      1.00        69\n",
      "           8       0.98      0.98      0.98        55\n",
      "\n",
      "    accuracy                           0.97       542\n",
      "   macro avg       0.97      0.97      0.97       542\n",
      "weighted avg       0.97      0.97      0.97       542\n",
      "\n",
      " The confusion matrix for version 4 data using support victor machine\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        66\n",
      "           1       1.00      0.95      0.98        44\n",
      "           2       0.97      1.00      0.99        73\n",
      "           3       0.90      0.97      0.93        58\n",
      "           4       1.00      0.95      0.97        60\n",
      "           5       0.92      0.92      0.92        60\n",
      "           6       0.98      0.95      0.96        57\n",
      "           7       1.00      1.00      1.00        69\n",
      "           8       0.98      0.98      0.98        55\n",
      "\n",
      "    accuracy                           0.97       542\n",
      "   macro avg       0.97      0.97      0.97       542\n",
      "weighted avg       0.97      0.97      0.97       542\n",
      "\n",
      " The confusion matrix for version 5 data using support victor machine\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        66\n",
      "           1       1.00      0.95      0.98        44\n",
      "           2       0.97      1.00      0.99        73\n",
      "           3       0.90      0.97      0.93        58\n",
      "           4       1.00      0.95      0.97        60\n",
      "           5       0.92      0.92      0.92        60\n",
      "           6       0.98      0.95      0.96        57\n",
      "           7       1.00      1.00      1.00        69\n",
      "           8       0.98      0.98      0.98        55\n",
      "\n",
      "    accuracy                           0.97       542\n",
      "   macro avg       0.97      0.97      0.97       542\n",
      "weighted avg       0.97      0.97      0.97       542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' The confusion matrix for version 1 data using support victor machine')\n",
    "print(classification_report(y_test1,prediction[\"svm1\"]))\n",
    "\n",
    "print(' The confusion matrix for version 2 data using support victor machine')\n",
    "print(classification_report(y_test2,prediction[\"svm1\"]))\n",
    "\n",
    "print(' The confusion matrix for version 3 data using support victor machine')\n",
    "print(classification_report(y_test3,prediction[\"svm1\"]))\n",
    "\n",
    "print(' The confusion matrix for version 4 data using support victor machine')\n",
    "print(classification_report(y_test4,prediction[\"svm1\"]))\n",
    "\n",
    "print(' The confusion matrix for version 5 data using support victor machine')\n",
    "print(classification_report(y_test5,prediction[\"svm1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The confusion matrix for version 1 data using Naive bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95        66\n",
      "           1       0.90      0.84      0.87        44\n",
      "           2       0.99      0.95      0.97        73\n",
      "           3       0.86      0.88      0.87        58\n",
      "           4       0.88      0.82      0.84        60\n",
      "           5       0.83      0.87      0.85        60\n",
      "           6       0.88      0.88      0.88        57\n",
      "           7       0.99      0.99      0.99        69\n",
      "           8       0.88      0.93      0.90        55\n",
      "\n",
      "    accuracy                           0.91       542\n",
      "   macro avg       0.90      0.90      0.90       542\n",
      "weighted avg       0.91      0.91      0.91       542\n",
      "\n",
      " The confusion matrix for version 2 data using Naive bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91        66\n",
      "           1       0.90      0.84      0.87        44\n",
      "           2       0.99      0.95      0.97        73\n",
      "           3       0.84      0.79      0.81        58\n",
      "           4       0.82      0.78      0.80        60\n",
      "           5       0.80      0.85      0.82        60\n",
      "           6       0.85      0.88      0.86        57\n",
      "           7       0.99      0.97      0.98        69\n",
      "           8       0.88      0.89      0.88        55\n",
      "\n",
      "    accuracy                           0.88       542\n",
      "   macro avg       0.88      0.88      0.88       542\n",
      "weighted avg       0.88      0.88      0.88       542\n",
      "\n",
      " The confusion matrix for version 3 data using Naive bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90        66\n",
      "           1       0.90      0.80      0.84        44\n",
      "           2       0.99      0.92      0.95        73\n",
      "           3       0.88      0.78      0.83        58\n",
      "           4       0.73      0.72      0.72        60\n",
      "           5       0.84      0.90      0.87        60\n",
      "           6       0.73      0.86      0.79        57\n",
      "           7       0.97      0.96      0.96        69\n",
      "           8       0.84      0.87      0.86        55\n",
      "\n",
      "    accuracy                           0.86       542\n",
      "   macro avg       0.86      0.86      0.86       542\n",
      "weighted avg       0.87      0.86      0.86       542\n",
      "\n",
      " The confusion matrix for version 4 data using Naive bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89        66\n",
      "           1       0.90      0.82      0.86        44\n",
      "           2       0.99      0.93      0.96        73\n",
      "           3       0.82      0.78      0.80        58\n",
      "           4       0.70      0.73      0.72        60\n",
      "           5       0.82      0.85      0.84        60\n",
      "           6       0.75      0.81      0.78        57\n",
      "           7       0.99      0.96      0.97        69\n",
      "           8       0.83      0.87      0.85        55\n",
      "\n",
      "    accuracy                           0.85       542\n",
      "   macro avg       0.85      0.85      0.85       542\n",
      "weighted avg       0.86      0.85      0.86       542\n",
      "\n",
      " The confusion matrix for version 5 data using Naive bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81        66\n",
      "           1       0.77      0.61      0.68        44\n",
      "           2       0.97      0.84      0.90        73\n",
      "           3       0.72      0.48      0.58        58\n",
      "           4       0.61      0.58      0.60        60\n",
      "           5       0.64      0.82      0.72        60\n",
      "           6       0.51      0.77      0.62        57\n",
      "           7       0.97      0.88      0.92        69\n",
      "           8       0.88      0.89      0.88        55\n",
      "\n",
      "    accuracy                           0.75       542\n",
      "   macro avg       0.76      0.74      0.75       542\n",
      "weighted avg       0.77      0.75      0.75       542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' The confusion matrix for version 1 data using Naive bayes')\n",
    "print(classification_report(y_test1,prediction[\"nb1\"]))\n",
    "\n",
    "print(' The confusion matrix for version 2 data using Naive bayes')\n",
    "print(classification_report(y_test2,prediction[\"nb2\"]))\n",
    "\n",
    "print(' The confusion matrix for version 3 data using Naive bayes')\n",
    "print(classification_report(y_test3,prediction[\"nb3\"]))\n",
    "\n",
    "print(' The confusion matrix for version 4 data using Naive bayes')\n",
    "print(classification_report(y_test4,prediction[\"nb4\"]))\n",
    "\n",
    "print(' The confusion matrix for version 5 data using Naive bayes')\n",
    "print(classification_report(y_test5,prediction[\"nb5\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The confusion matrix for version 1 data using Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84        66\n",
      "           1       0.88      0.82      0.85        44\n",
      "           2       0.86      0.77      0.81        73\n",
      "           3       0.70      0.76      0.73        58\n",
      "           4       0.71      0.68      0.69        60\n",
      "           5       0.77      0.78      0.78        60\n",
      "           6       0.74      0.65      0.69        57\n",
      "           7       0.94      0.94      0.94        69\n",
      "           8       0.77      0.84      0.80        55\n",
      "\n",
      "    accuracy                           0.80       542\n",
      "   macro avg       0.79      0.79      0.79       542\n",
      "weighted avg       0.80      0.80      0.79       542\n",
      "\n",
      " The confusion matrix for version 2 data using Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82        66\n",
      "           1       0.68      0.73      0.70        44\n",
      "           2       0.83      0.78      0.80        73\n",
      "           3       0.72      0.72      0.72        58\n",
      "           4       0.75      0.72      0.74        60\n",
      "           5       0.79      0.63      0.70        60\n",
      "           6       0.76      0.72      0.74        57\n",
      "           7       0.91      0.88      0.90        69\n",
      "           8       0.70      0.85      0.77        55\n",
      "\n",
      "    accuracy                           0.77       542\n",
      "   macro avg       0.77      0.77      0.77       542\n",
      "weighted avg       0.78      0.77      0.77       542\n",
      "\n",
      " The confusion matrix for version 3 data using Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83        66\n",
      "           1       0.75      0.75      0.75        44\n",
      "           2       0.78      0.78      0.78        73\n",
      "           3       0.72      0.66      0.68        58\n",
      "           4       0.70      0.75      0.73        60\n",
      "           5       0.57      0.67      0.62        60\n",
      "           6       0.68      0.60      0.64        57\n",
      "           7       0.85      0.88      0.87        69\n",
      "           8       0.87      0.82      0.84        55\n",
      "\n",
      "    accuracy                           0.75       542\n",
      "   macro avg       0.75      0.75      0.75       542\n",
      "weighted avg       0.75      0.75      0.75       542\n",
      "\n",
      " The confusion matrix for version 4 data using Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.71        66\n",
      "           1       0.69      0.77      0.73        44\n",
      "           2       0.89      0.70      0.78        73\n",
      "           3       0.71      0.81      0.76        58\n",
      "           4       0.60      0.68      0.64        60\n",
      "           5       0.56      0.57      0.56        60\n",
      "           6       0.71      0.53      0.61        57\n",
      "           7       0.85      0.93      0.89        69\n",
      "           8       0.70      0.78      0.74        55\n",
      "\n",
      "    accuracy                           0.72       542\n",
      "   macro avg       0.72      0.72      0.71       542\n",
      "weighted avg       0.73      0.72      0.72       542\n",
      "\n",
      " The confusion matrix for version 5 data using Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77        66\n",
      "           1       0.69      0.77      0.73        44\n",
      "           2       0.81      0.77      0.79        73\n",
      "           3       0.68      0.76      0.72        58\n",
      "           4       0.74      0.72      0.73        60\n",
      "           5       0.73      0.62      0.67        60\n",
      "           6       0.81      0.67      0.73        57\n",
      "           7       0.91      0.93      0.92        69\n",
      "           8       0.64      0.78      0.70        55\n",
      "\n",
      "    accuracy                           0.76       542\n",
      "   macro avg       0.75      0.75      0.75       542\n",
      "weighted avg       0.76      0.76      0.76       542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' The confusion matrix for version 1 data using Decision Tree')\n",
    "print(classification_report(y_test1,prediction[\"dt1\"]))\n",
    "\n",
    "print(' The confusion matrix for version 2 data using Decision Tree')\n",
    "print(classification_report(y_test2,prediction[\"dt2\"]))\n",
    "\n",
    "print(' The confusion matrix for version 3 data using Decision Tree')\n",
    "print(classification_report(y_test3,prediction[\"dt3\"]))\n",
    "\n",
    "print(' The confusion matrix for version 4 data using Decision Tree')\n",
    "print(classification_report(y_test4,prediction[\"dt4\"]))\n",
    "\n",
    "print(' The confusion matrix for version 5 data using Decision Tree')\n",
    "print(classification_report(y_test5,prediction[\"dt5\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
